{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrase Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수정사항 \n",
    "\n",
    "- norm_alice = list(filter(None, normalize_corpus(alice, lemmatize=False)))\n",
    "- lib/python3.6/site-packages/nltk/tag/__init__.py\n",
    "  - pos_tag_sents\n",
    "    - return [_pos_tag(sent, tagset, tagger, lang) for sent in sentences] # line:180\n",
    "    \n",
    "- colab :\n",
    "  - sudo apt-get install ibmysqlclient-dev\n",
    "  - pip install --upgrade pattern\n",
    "- nltk :\n",
    "  - nltk.download('stopwords')\n",
    "  - nltk.downalod('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /Users/leo/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from normalization import normalize_corpus\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "alice = [' '.join(ts) for ts in alice]\n",
    "norm_alice = list(filter(None, normalize_corpus(alice, lemmatize=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice adventures wonderland lewis carroll 1865\n"
     ]
    }
   ],
   "source": [
    "# print first line\n",
    "print (norm_alice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip() \n",
    "                     for document in corpus])\n",
    "                         \n",
    "def compute_ngrams(sequence, n):\n",
    "    return zip(*[sequence[index:] \n",
    "                 for index in range(n)])\n",
    "\n",
    "\n",
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq) \n",
    "                     for text, freq in sorted_ngrams]\n",
    "\n",
    "    return sorted_ngrams   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said alice', 123),\n",
       " ('mock turtle', 56),\n",
       " ('march hare', 31),\n",
       " ('said king', 29),\n",
       " ('thought alice', 26),\n",
       " ('white rabbit', 22),\n",
       " ('said hatter', 22),\n",
       " ('said mock', 20),\n",
       " ('said caterpillar', 18),\n",
       " ('said gryphon', 18)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus=norm_alice, ngram_val=2,limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said mock turtle', 20),\n",
       " ('said march hare', 10),\n",
       " ('poor little thing', 6),\n",
       " ('little golden key', 5),\n",
       " ('certainly said alice', 5),\n",
       " ('white kid gloves', 5),\n",
       " ('march hare said', 5),\n",
       " ('mock turtle said', 5),\n",
       " ('know said alice', 4),\n",
       " ('might well say', 4)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus=norm_alice, ngram_val=3,limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 462), ('alice', 398), ('little', 128), ('one', 104), ('know', 88)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus=norm_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'alice'),\n",
       " ('mock', 'turtle'),\n",
       " ('march', 'hare'),\n",
       " ('said', 'king'),\n",
       " ('thought', 'alice'),\n",
       " ('said', 'hatter'),\n",
       " ('white', 'rabbit'),\n",
       " ('said', 'mock'),\n",
       " ('said', 'caterpillar'),\n",
       " ('said', 'gryphon')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in norm_alice])\n",
    "bigram_measures = BigramAssocMeasures()                                                \n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abide', 'figures'),\n",
       " ('acceptance', 'elegant'),\n",
       " ('accounting', 'tastes'),\n",
       " ('accustomed', 'usurpation'),\n",
       " ('act', 'crawling'),\n",
       " ('adjourn', 'immediate'),\n",
       " ('adoption', 'energetic'),\n",
       " ('affair', 'trusts'),\n",
       " ('agony', 'terror'),\n",
       " ('alarmed', 'proposal')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(bigram_measures.pmi, 10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'mock', 'turtle'),\n",
       " ('said', 'march', 'hare'),\n",
       " ('poor', 'little', 'thing'),\n",
       " ('little', 'golden', 'key'),\n",
       " ('march', 'hare', 'said'),\n",
       " ('mock', 'turtle', 'said'),\n",
       " ('white', 'kid', 'gloves'),\n",
       " ('beau', 'ootiful', 'soo'),\n",
       " ('certainly', 'said', 'alice'),\n",
       " ('might', 'well', 'say')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in norm_alice])\n",
    "trigram_measures = TrigramAssocMeasures()                                                \n",
    "finder.nbest(trigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accustomed', 'usurpation', 'conquest'),\n",
       " ('adjourn', 'immediate', 'adoption'),\n",
       " ('adoption', 'energetic', 'remedies'),\n",
       " ('ancient', 'modern', 'seaography'),\n",
       " ('apple', 'roast', 'turkey'),\n",
       " ('arithmetic', 'ambition', 'distraction'),\n",
       " ('brother', 'latin', 'grammar'),\n",
       " ('canvas', 'bag', 'tied'),\n",
       " ('cherry', 'tart', 'custard'),\n",
       " ('circle', 'exact', 'shape')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(trigram_measures.pmi, 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toy_text = \"\"\"\n",
    "Elephants are large mammals of the family Elephantidae \n",
    "and the order Proboscidea. Two species are traditionally recognised, \n",
    "the African elephant and the Asian elephant. Elephants are scattered \n",
    "throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male \n",
    "African elephants are the largest extant terrestrial animals. All \n",
    "elephants have a long trunk used for many purposes, \n",
    "particularly breathing, lifting water and grasping objects. Their \n",
    "incisors grow into tusks, which can serve as weapons and as tools \n",
    "for moving objects and digging. Elephants' large ear flaps help \n",
    "to control their body temperature. Their pillar-like legs can \n",
    "carry their great weight. African elephants have larger ears \n",
    "and concave backs while Asian elephants have smaller ears \n",
    "and convex or level backs.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization import parse_document\n",
    "import itertools\n",
    "import nltk\n",
    "from normalization import stopword_list\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}'):\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        tagged_sents = nltk.pos_tag_sents(\n",
    "                            [nltk.word_tokenize(sentence)], lang='eng')\n",
    "        \n",
    "        chunks = [chunker.parse(tagged_sent) \n",
    "                  for tagged_sent in tagged_sents]\n",
    "        \n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                     for chunk in chunks]    \n",
    "         \n",
    "        flattened_chunks = list(\n",
    "                            itertools.chain.from_iterable(\n",
    "                                wtc_sent for wtc_sent in wtc_sents)\n",
    "                           )\n",
    "        \n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n",
    "                        for status, chunk \n",
    "                        in itertools.groupby(flattened_chunks, \n",
    "                                             lambda chunk: chunk[2] != 'O')]\n",
    "        \n",
    "        valid_chunks = [' '.join(word.lower() \n",
    "                                for word, tag, chunk \n",
    "                                in wtc_group \n",
    "                                    if word.lower() \n",
    "                                        not in stopword_list) \n",
    "                                    for status, wtc_group \n",
    "                                    in valid_chunks_tagged\n",
    "                                        if status]\n",
    "                                            \n",
    "        all_chunks.append(valid_chunks)\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = parse_document(toy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_chunks = get_chunks(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['elephants', 'large mammals', 'family elephantidae', 'order proboscidea'], ['species', 'african elephant', 'asian elephant'], ['elephants', 'sub-saharan africa', 'south asia', 'southeast asia'], ['male african elephants', 'extant terrestrial animals'], ['elephants', 'long trunk', 'many purposes', 'breathing', 'water', 'grasping objects'], ['incisors', 'tusks', 'weapons', 'tools', 'objects', 'digging'], ['elephants', 'large ear flaps', 'body temperature'], ['pillar-like legs', 'great weight'], ['african elephants', 'ears', 'backs', 'asian elephants', 'ears', 'convex', 'level backs']]\n"
     ]
    }
   ],
   "source": [
    "print (valid_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_weighted_keyphrases(sentences, \n",
    "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
    "                                  top_n=10):\n",
    "    \n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
    "                                     \n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    weighted_phrases = {dictionary.get(id): round(value,3) \n",
    "                        for doc in corpus_tfidf \n",
    "                        for id, value in doc}\n",
    "                            \n",
    "    weighted_phrases = sorted(weighted_phrases.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    return weighted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('extant terrestrial animals', 0.707),\n",
       " ('male african elephants', 0.707),\n",
       " ('great weight', 0.707),\n",
       " ('pillar-like legs', 0.707),\n",
       " ('body temperature', 0.684),\n",
       " ('large ear flaps', 0.684),\n",
       " ('ears', 0.667),\n",
       " ('african elephant', 0.577),\n",
       " ('asian elephant', 0.577),\n",
       " ('species', 0.577)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tfidf_weighted_keyphrases(sentences, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chapter', 1.0),\n",
       " ('rabbit', 1.0),\n",
       " ('thought alice', 1.0),\n",
       " ('sides', 1.0),\n",
       " ('ignorant little girl', 1.0),\n",
       " ('dinah', 1.0),\n",
       " ('thump', 1.0),\n",
       " ('whiskers', 1.0),\n",
       " ('* * * * * * *', 1.0),\n",
       " ('* * * * * *', 1.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try on other corpora!\n",
    "get_tfidf_weighted_keyphrases(alice, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
