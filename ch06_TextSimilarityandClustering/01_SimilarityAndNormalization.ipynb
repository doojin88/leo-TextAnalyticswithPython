{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter06. Text Similarity and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measure\n",
    "\n",
    "참고 : The paper by A. Huang, “Similarity Measures for Text Document Clustering,” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several distance measures that measure similarity, and we will be covering several of them in future sections. \n",
    "\n",
    "However, an important thing to remember is that all distance measures of similarity are not distance metrics of similarity. \n",
    "\n",
    "The excellent paper by A. Huang, “Similarity Measures for Text Document Clustering,” talks about this in detail. \n",
    "\n",
    "Consider a distance measure *d* and two entities (say they are documents in our context) *x* and *y*. \n",
    "\n",
    "The distance between *x* and *y*, which is used to determine the degree of similarity between them, can be represented as *d*(*x*, *y*), but the measure *d* can be called as a *distance metric of similarity* if and only if it satisfies the following four conditions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>dstance metric을 similarity metric 으로 사용할 때 필요한 조건:</h2>\n",
    "\n",
    "Distance $d(x,y)$ : <br/>\n",
    "\n",
    "- The distance between x and y, which is used to determine the degree of similarity between them, can be represented as $d(x,y)$, \n",
    "\n",
    "<br/>\n",
    "\n",
    "필요한 조건 : the measure d can be called as a **distance metric of similarity** if and only if it satisfies the following four conditions:\n",
    "\n",
    "1. The distance measured between any two entities, say `x` and `y`, must be always **non-negative**, that is,\n",
    "$$ \\large d\\left(x,\\ y\\right)\\ge 0 $$\n",
    "<br/>\n",
    "2. The distance between two entities should always be **zero** if and only if they are both identical, that is, \n",
    "$$ \\large d\\left(x,\\ y\\right) = 0\\ iff\\ x=y $$\n",
    "<br/>\n",
    "3. This distance measure should always be **symmetric**, which means that the distance from `x` to `y` is always the same as the distance from `y` to `x`. \n",
    "<br/>\n",
    "Mathematically this is represented as \n",
    "$$ \\large d\\left(x,\\ y\\right) = d\\left(y,\\ x\\right) $$\n",
    "<br/>\n",
    "4. This distance measure should satisfy the **triangle inequality** property, which can be mathematically represented \n",
    "$$ \\large d\\left(x,\\ z\\right)\\le\\ d\\left(x,\\ y\\right) + d\\left(y,\\ z\\right) $$\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to normalize our text documents and corpora as usual before we perform any further analyses or NLP. \n",
    "\n",
    "For this we will reuse our normalization module from Chapter [5](https://learning.oreilly.com/library/view/text-analytics-with/9781484223871/A427287_1_En_5_Chapter.html) but with a few more additions specifically aimed toward this chapter. \n",
    "\n",
    "The complete normalization module is available in the code files for this chapter in the file **normalization.py**, \n",
    "\n",
    "but I will still be highlighting the new additions in our normalization module in this section for your benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **main steps** performed in text normalization include the following:\n",
    "\n",
    "1. Sentence extraction\n",
    "2. Unescape HTML escape sequences\n",
    "3. Expand contractions\n",
    "4. Lemmatize text\n",
    "5. Remove special characters\n",
    "6. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from html.parser import HTMLParser\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords\n",
    "\n",
    "we have updated our stopwords list with several new words that have been carefully selected after analyzing many corpora. \n",
    "\n",
    "The following code snippet illustrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get', 'tell', 'listen', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero', 'join', 'find', 'make', 'say', 'ask', 'tell', 'see', 'try', 'back', 'also']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization import tokenize_text, expand_contractions, lemmatize_text, remove_special_characters, remove_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주의**: HTMLParser import 가 안되는 경우 **normalization.py** 파일을 아래와 같이 변경해 준다.\n",
    "\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep_text_characters\n",
    "\n",
    "We also add a new function in our normalization pipeline, which is to only extract text tokens from a body of text for which we use regular expressions, as depicted in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from normalization import keep_text_characters\n",
    "\n",
    "import re\n",
    "\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In late summer guests are gathered for the wedding reception of Don Vito Corleone 's daughter Connie Talia Shire and Carlo Rizzi Gianni Russo Vito Marlon Brando the head of the Corleone Mafia family is known to friends and associates as Godfather He and Tom Hagen Robert Duvall the Corleone family lawyer are hearing requests for favors because according to Italian tradition no Sicilian can refuse a request on his daughter 's wedding day\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movie_data.csv: The Godfather\n",
    "leo_text = '''\n",
    "In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone's daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando), the head of the Corleone Mafia family, is known to friends and associates as \"\"Godfather.\"\" He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors because, according to Italian tradition, \"\"no Sicilian can refuse a request on his daughter's wedding day.\n",
    "'''\n",
    "text = keep_text_characters(leo_text);\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize_corpus (updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from normalization import normalize_corpus\n",
    "def normalize_corpus(corpus, lemmatize=True,\n",
    "                     only_text_chars=False,\n",
    "                     tokenize=False):\n",
    "\n",
    "    normalized_corpus = []    \n",
    "    for text in corpus:\n",
    "        text = html_parser.unescape(text)\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        if lemmatize:                                                                      \n",
    "            text = lemmatize_text(text)\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if only_text_chars:\n",
    "            text = keep_text_characters(text)\n",
    "\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "\n",
    "    return normalized_corpus                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_corpus from document_similarity.py\n",
    "toy_corpus = [\n",
    "'The sky is blue',\n",
    "'The sky is blue and beautiful',\n",
    "'Look at the bright blue sky!',\n",
    "'Python is a great Programming language',\n",
    "'Python and Java are popular Programming languages',\n",
    "'Among Programming languages, both Python and Java are the most used in Analytics',\n",
    "'The fox is quicker than the lazy dog',\n",
    "'The dog is smarter than the fox',\n",
    "'The dog, fox and cat are good friends'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_docs = ['The fox is definitely smarter than the dog',\n",
    "            'Java is a static typed programming language unlike Python',\n",
    "            'I love to relax under the beautiful blue sky!']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'blue'],\n",
       " ['sky', 'blue', 'beautiful'],\n",
       " ['look', 'bright', 'blue', 'sky'],\n",
       " ['python', 'great', 'programming', 'language'],\n",
       " ['python', 'java', 'popular', 'programming', 'language'],\n",
       " ['among', 'programming', 'language', 'python', 'java', 'use', 'analytics'],\n",
       " ['fox', 'quick', 'lazy', 'dog'],\n",
       " ['dog', 'smarter', 'fox'],\n",
       " ['dog', 'fox', 'cat', 'good', 'friend']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(toy_corpus, tokenize=True)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "parameters: \n",
    "\n",
    "\n",
    "- feature types:\n",
    "  - binary : Bag of Words occurrences,\n",
    "  - frequency : Bag of Words frequency,\n",
    "  - tfidf : TF-IDF–based features.\n",
    "- ngram_range : \n",
    "  - bigrams, trigrams, and so on.\n",
    "- min_df : [0.0, 1.0], \n",
    "  - lower bound document frequency threshold value.\n",
    "- max_df : [0.0, 1.0], \n",
    "  - upper bound document frequency threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import build_feature_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type='frequency', ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "\n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency','tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "\n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### norm_corpus (toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(toy_corpus, lemmatize=True)\n",
    "\n",
    "vectorizer, corpus_features = build_feature_matrix(norm_corpus, feature_type='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9x22 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### norm_query_docs (query_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_query_docs =  normalize_corpus(query_docs, lemmatize=True)  \n",
    "\n",
    "query_docs_features = vectorizer.transform(norm_query_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x22 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_docs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 19)\t1\n"
     ]
    }
   ],
   "source": [
    "for index, doc in enumerate(query_docs):\n",
    "    doc_features = query_docs_features[index]\n",
    "    print(doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
