{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter06. Text Similarity and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measure\n",
    "\n",
    "참고 : \n",
    "- The paper by A. Huang, “Similarity Measures for Text Document Clustering,” \n",
    "\n",
    "Consider a distance measure *d* and two entities (say they are documents in our context) *x* and *y*.\n",
    "\n",
    "<br/>\n",
    "\n",
    "The distance between *x* and *y*, which is used to determine the degree of similarity between them, can be represented as *d*(*x*, *y*), but the measure *d* can be called as a *distance metric of similarity* if and only if it satisfies the following four conditions:\n",
    "\n",
    "1. The distance measured between any two entities, say *x* and *y*, must be always non-negative, that is, ![$$ d\\left(x,\\ y\\right)\\ge 0 $$](https://learning.oreilly.com/library/view/text-analytics-with/9781484223871/A427287_1_En_6_Chapter_IEq1.gif).\n",
    "2. The distance between two entities should always be zero if and only if they are both identical, that is, ![$$ d\\left(x,\\ y\\right)\\ge 0\\ iff\\ x=y $$](https://learning.oreilly.com/library/view/text-analytics-with/9781484223871/A427287_1_En_6_Chapter_IEq2.gif).\n",
    "3. This distance measure should always be symmetric, which means that the distance from *x* to *y* is always the same as the distance from *y* to *x*. Mathematically this is represented as ![$$ d\\left(x,\\ y\\right) = d\\left(y,\\ x\\right) $$](https://learning.oreilly.com/library/view/text-analytics-with/9781484223871/A427287_1_En_6_Chapter_IEq3.gif).\n",
    "4. This distance measure should satisfy the *triangle inequality* property, which can be mathematically represented ![$$ d\\left(x,\\ z\\right)\\le\\ d\\left(x,\\ y\\right) + d\\left(y,\\ z\\right) $$](https://learning.oreilly.com/library/view/text-analytics-with/9781484223871/A427287_1_En_6_Chapter_IEq4.gif)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from html.parser import HTMLParser\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get', 'tell', 'listen', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero', 'join', 'find', 'make', 'say', 'ask', 'tell', 'see', 'try', 'back', 'also']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주의**: HTMLParser import 가 안되는 경우 **normalization.py** 파일을 아래와 같이 변경해 준다.\n",
    "\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization import tokenize_text, expand_contractions, lemmatize_text, remove_special_characters, remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from normalization import keep_text_characters\n",
    "\n",
    "import re\n",
    "\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In late summer guests are gathered for the wedding reception of Don Vito Corleone 's daughter Connie Talia Shire and Carlo Rizzi Gianni Russo Vito Marlon Brando the head of the Corleone Mafia family is known to friends and associates as Godfather He and Tom Hagen Robert Duvall the Corleone family lawyer are hearing requests for favors because according to Italian tradition no Sicilian can refuse a request on his daughter 's wedding day\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movie_data.csv: The Godfather\n",
    "leo_text = '''\n",
    "In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone's daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando), the head of the Corleone Mafia family, is known to friends and associates as \"\"Godfather.\"\" He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors because, according to Italian tradition, \"\"no Sicilian can refuse a request on his daughter's wedding day.\n",
    "'''\n",
    "text = keep_text_characters(leo_text);\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from normalization import normalize_corpus\n",
    "def normalize_corpus(corpus, lemmatize=True,\n",
    "                     only_text_chars=False,\n",
    "                     tokenize=False):\n",
    "\n",
    "    normalized_corpus = []    \n",
    "    for text in corpus:\n",
    "        text = html_parser.unescape(text)\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        if lemmatize:                                                                      \n",
    "            text = lemmatize_text(text)\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if only_text_chars:\n",
    "            text = keep_text_characters(text)\n",
    "\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "\n",
    "    return normalized_corpus                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_corpus from document_similarity.py\n",
    "toy_corpus = [\n",
    "'The sky is blue',\n",
    "'The sky is blue and beautiful',\n",
    "'Look at the bright blue sky!',\n",
    "'Python is a great Programming language',\n",
    "'Python and Java are popular Programming languages',\n",
    "'Among Programming languages, both Python and Java are the most used in Analytics',\n",
    "'The fox is quicker than the lazy dog',\n",
    "'The dog is smarter than the fox',\n",
    "'The dog, fox and cat are good friends'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_docs = ['The fox is definitely smarter than the dog',\n",
    "            'Java is a static typed programming language unlike Python',\n",
    "            'I love to relax under the beautiful blue sky!']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['sky', 'blue'],\n",
       " ['sky', 'blue', 'beautiful'],\n",
       " ['look', 'bright', 'blue', 'sky'],\n",
       " ['python', 'great', 'programming', 'language'],\n",
       " ['python', 'java', 'popular', 'programming', 'language'],\n",
       " ['among', 'programming', 'language', 'python', 'java', 'use', 'analytics'],\n",
       " ['fox', 'quick', 'lazy', 'dog'],\n",
       " ['dog', 'smarter', 'fox'],\n",
       " ['dog', 'fox', 'cat', 'good', 'friend']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(toy_corpus, tokenize=True)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['late',\n",
       "  'summer',\n",
       "  '1945',\n",
       "  'guest',\n",
       "  'gather',\n",
       "  'wedding',\n",
       "  'reception',\n",
       "  'vito',\n",
       "  'corleones',\n",
       "  'daughter',\n",
       "  'connie',\n",
       "  'talia',\n",
       "  'shire',\n",
       "  'carlo',\n",
       "  'rizzi',\n",
       "  'gianni',\n",
       "  'russo',\n",
       "  'vito',\n",
       "  'marlon',\n",
       "  'brando',\n",
       "  'head',\n",
       "  'corleone',\n",
       "  'mafia',\n",
       "  'family',\n",
       "  'know',\n",
       "  'friend',\n",
       "  'associate',\n",
       "  'godfather',\n",
       "  'tom',\n",
       "  'hagen',\n",
       "  'robert',\n",
       "  'duvall',\n",
       "  'corleone',\n",
       "  'family',\n",
       "  'lawyer',\n",
       "  'hearing',\n",
       "  'request',\n",
       "  'favor',\n",
       "  'accord',\n",
       "  'italian',\n",
       "  'tradition',\n",
       "  'sicilian',\n",
       "  'refuse',\n",
       "  'request',\n",
       "  'daughter',\n",
       "  'wedding',\n",
       "  'day']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leo_corpus = [leo_text]\n",
    "norm_leo_corpus = normalize_corpus(leo_corpus, tokenize=True)\n",
    "norm_leo_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature types:\n",
    "- Bag of Words frequency, occurrences,\n",
    "- TF-IDF–based features.\n",
    "\n",
    "parameters: \n",
    "\n",
    "- ngram_range : bigrams, trigrams, and so on.\n",
    "- min_df : [0.0, 1.0], lower bound document frequency threshold value.\n",
    "- max_df : [0.0, 1.0], upper bound document frequency threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import build_feature_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type='frequency', ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "\n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency','tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "\n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(toy_corpus, lemmatize=True)\n",
    "\n",
    "vectorizer, corpus_features = build_feature_matrix(norm_corpus, feature_type='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9x22 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "norm_query_docs =  normalize_corpus(query_docs, lemmatize=True)  \n",
    "\n",
    "query_docs_features = vectorizer.transform(norm_query_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x22 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_docs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 19)\t1\n"
     ]
    }
   ],
   "source": [
    "for index, doc in enumerate(query_docs):\n",
    "    doc_features = query_docs_features[index]\n",
    "    print(doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
