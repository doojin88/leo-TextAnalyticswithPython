{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity\n",
    "\n",
    "\n",
    "- Cosine similarity\n",
    "- Hellinger-Bhattacharya distance\n",
    "- Okapi BM25 ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with loading the necessary dependencies and the corpus of documents on which we will be testing our various metrics, as shown in the following code snippet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization import normalize_corpus\n",
    "from utils import build_feature_matrix\n",
    "import numpy as np\n",
    "\n",
    "# load the toy corpus index\n",
    "toy_corpus = [\n",
    "    'The sky is blue',\n",
    "    'The sky is blue and beautiful',\n",
    "    'Look at the bright blue sky!',\n",
    "    'Python is a great Programming language',\n",
    "    'Python and Java are popular Programming languages',\n",
    "    'Among Programming languages, both Python and Java are the most used in Analytics',\n",
    "    'The fox is quicker than the lazy dog',\n",
    "    'The dog is smarter than the fox',\n",
    "    'The dog, fox and cat are good friends'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and extract features from the toy corpus\n",
    "norm_corpus = normalize_corpus(toy_corpus, lemmatize=True)\n",
    "\n",
    "tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpus,\n",
    "                                                        feature_type='tfidf',\n",
    "                                                        ngram_range=(1, 1),\n",
    "                                                        min_df=0.0, max_df=1.0)                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.70710678 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70710678 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_features[0].toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the docs for which we will be measuring similarities\n",
    "query_docs = [\n",
    "    'The fox is definitely smarter than the dog',\n",
    "    'Java is a static typed programming language unlike Python',\n",
    "    'I love to relax under the beautiful blue sky!'\n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and extract features from the query corpus\n",
    "norm_query_docs =  normalize_corpus(query_docs, lemmatize=True)\n",
    "\n",
    "query_docs_tfidf = tfidf_vectorizer.transform(norm_query_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.50936532 0.50936532 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.69360936 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(query_docs_tfidf[0].toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSINE SIMILARITY\n",
    "\n",
    "Document :\n",
    "- The document vectors will be the Bag of Words model–based vectors with TF-IDF values instead of term frequencies. \n",
    "\n",
    "Cosine similarity : \n",
    "\n",
    "$$ cs\\left(u,\\ v\\right)= \\cos \\left(\\theta \\right) = \\frac{u\\cdot v}{\\left|\\left|u\\left|\\right|\\kern0.5em \\left|\\right|v\\right|\\right|} = \\frac{{\\displaystyle {\\sum}_{i=1}^n}{u}_i\\ {v}_i}{\\sqrt{{\\displaystyle {\\sum}_{i=1}^n}{u}_i^2}\\ \\sqrt{{\\displaystyle {\\sum}_{i=1}^n}{v}_i^2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(doc_features, corpus_features, top_n=3):\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    \n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features, corpus_features.T)\n",
    "    \n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3)) for index in top_docs]\n",
    "    \n",
    "    return top_docs_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Analysis using Cosine Similarity\n",
      "============================================================\n",
      "Document 1 : The fox is definitely smarter than the dog\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 8\n",
      "  Similarity Score: 1.0\n",
      "  Doc : The dog is smarter than the fox\n",
      "----------------------------------------\n",
      "  Doc num: 7\n",
      "  Similarity Score: 0.426\n",
      "  Doc : The fox is quicker than the lazy dog\n",
      "----------------------------------------\n",
      "\n",
      "Document 2 : Java is a static typed programming language unlike Python\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 5\n",
      "  Similarity Score: 0.837\n",
      "  Doc : Python and Java are popular Programming languages\n",
      "----------------------------------------\n",
      "  Doc num: 6\n",
      "  Similarity Score: 0.661\n",
      "  Doc : Among Programming languages, both Python and Java are the most used in Analytics\n",
      "----------------------------------------\n",
      "\n",
      "Document 3 : I love to relax under the beautiful blue sky!\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 2\n",
      "  Similarity Score: 1.0\n",
      "  Doc : The sky is blue and beautiful\n",
      "----------------------------------------\n",
      "  Doc num: 1\n",
      "  Similarity Score: 0.72\n",
      "  Doc : The sky is blue\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Document Similarity Analysis using Cosine Similarity')\n",
    "print ('='*60)\n",
    "for index, doc in enumerate(query_docs):    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    top_similar_docs = compute_cosine_similarity(doc_tfidf, tfidf_features, top_n=2)\n",
    "    print ('Document',index+1 ,':', doc)\n",
    "    print ('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print ('-'*40)\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        print ('  Doc num: {}'.format(doc_index+1))\n",
    "        print ('  Similarity Score: {}'.format(sim_score))\n",
    "        print ('  Doc : {}'.format(toy_corpus[doc_index]))\n",
    "        print ('-'*40)\n",
    "    print ('') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELLINGER-BHATTACHARYA DISTANCE\n",
    "\n",
    "The *Hellinger-Bhattacharya distance* (*HB-distance*) is also called the *Hellinger distance* or the *Bhattacharya distance*. \n",
    "\n",
    "참고 : \n",
    "- https://en.wikipedia.org/wiki/Hellinger_distance\n",
    "- https://en.wikipedia.org/wiki/Bhattacharyya_distance\n",
    "- https://stats.stackexchange.com/questions/130432/differences-between-bhattacharyya-distance-and-kl-divergence\n",
    "\n",
    "**Hellinger-Bhattacharya distance** : \n",
    "- We will be using the TF-IDF–based vectors as our document distributions. \n",
    "- This makes it discrete distributions because we have specific TF-IDF values for specific feature terms, unlike continuous distributions.\n",
    "- We can define the Hellinger-Bhattacharya distance mathematically as :\n",
    "  - $\\large hbd\\left(u,\\ v\\right) = \\frac{1}{\\sqrt{2}}\\ \\left|\\right|\\sqrt{u} - \\sqrt{v}\\left|\\right|{}_2 $  \n",
    "    - where *hbd*(`u`, `v`) denotes the Hellinger-Bhattacharya distance between the document vectors `u` and `v`, \n",
    "    - and it is equal to the Euclidean or L2 norm of the difference of the square root of the vectors divided by the square root of 2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HB-distance converted formula** : \n",
    "\n",
    "Considering the document vectors `u` and `v` to be discrete with `n` number of features,\n",
    "\n",
    "we can further expand the above formula into  :\n",
    "\n",
    "- $ \\large {hbd}\\left(u,\\ v\\right) = \\frac{1}{\\sqrt{2}}\\ \\sqrt{{\\displaystyle \\sum_{i=1}^n}{\\left(\\sqrt{u_i} - \\sqrt{v_i}\\right)}^2} $\n",
    "  - where :\n",
    "    - $ u=\\left({u}_1,\\ {u}_2,\\dots,\\ {u}_n\\right) $\n",
    "    - $ v=\\left({v}_1,\\ {v}_2,\\dots,\\ {v}_n\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hellinger_bhattacharya_distance(doc_features, corpus_features, top_n=3):\n",
    "    # get document vectors                                            \n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    \n",
    "    # compute hb distances\n",
    "    distance = np.hstack(  np.sqrt(0.5 * np.sum(  np.square(np.sqrt(doc_features) -  np.sqrt(corpus_features)),  axis=1) ) )\n",
    "    \n",
    "    # get docs with lowest distance scores                            \n",
    "    top_docs = distance.argsort()[:top_n]\n",
    "    top_docs_with_score = [(index, round(distance[index], 3)) for index in top_docs]\n",
    "    \n",
    "    return top_docs_with_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Analysis using Hellinger-Bhattacharya distance\n",
      "============================================================\n",
      "Document 1 : The fox is definitely smarter than the dog\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 8\n",
      "  Distance Score: 0.0\n",
      "  Doc : The dog is smarter than the fox\n",
      "----------------------------------------\n",
      "  Doc num: 7\n",
      "  Distance Score: 0.96\n",
      "  Doc : The fox is quicker than the lazy dog\n",
      "----------------------------------------\n",
      "\n",
      "Document 2 : Java is a static typed programming language unlike Python\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 5\n",
      "  Distance Score: 0.53\n",
      "  Doc : Python and Java are popular Programming languages\n",
      "----------------------------------------\n",
      "  Doc num: 4\n",
      "  Distance Score: 0.766\n",
      "  Doc : Python is a great Programming language\n",
      "----------------------------------------\n",
      "\n",
      "Document 3 : I love to relax under the beautiful blue sky!\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 2\n",
      "  Distance Score: 0.0\n",
      "  Doc : The sky is blue and beautiful\n",
      "----------------------------------------\n",
      "  Doc num: 1\n",
      "  Distance Score: 0.602\n",
      "  Doc : The sky is blue\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Document Similarity Analysis using Hellinger-Bhattacharya distance')\n",
    "print ('='*60)\n",
    "for index, doc in enumerate(query_docs):\n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    top_similar_docs = compute_hellinger_bhattacharya_distance(doc_tfidf,  tfidf_features,  top_n=2)\n",
    "    \n",
    "    print ('Document',index+1 ,':', doc)\n",
    "    print ('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print ('-'*40)\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        print ('  Doc num: {}'.format(doc_index+1))\n",
    "        print ('  Distance Score: {}'.format(sim_score))\n",
    "        print ('  Doc : {}'.format(toy_corpus[doc_index]))\n",
    "        print ('-'*40)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKAPI BM25 RANKING\n",
    "\n",
    "### Document Ranking\n",
    "\n",
    "There are several techniques that are quite popular in information retrieval and search engines, including PageRank and Okapi BM25.\n",
    "\n",
    "The acronym *BM* stands for *best matching*. \n",
    "\n",
    "This technique is also known as BM25, but for the sake of completeness I refer to it as Okapi BM25, \n",
    "because originally although the concepts behind the BM25 function were merely theoretical, \n",
    "the City University in London built the Okapi Information Retrieval system in the 1980s–90s, \n",
    "which implemented this technique to retrieve documents on actual real-world data.\n",
    "\n",
    "### Okapi BM25 \n",
    "The Okapi BM25 can be formally defined as a **document ranking** and retrieval function based on a **Bag of Words–based** model for retrieving relevant documents based on a user input query.\n",
    "\n",
    "**BM25 score** :\n",
    "\n",
    "Assuming we have these, we can mathematically define the BM25 score between these two documents as :\n",
    "$ \\large bm25\\left(CD,\\ QD\\right) = {\\displaystyle \\sum_{i=1}^n}idf\\left({q}_i\\right)\\cdot \\frac{f\\left({q}_i,\\kern0.5em CD\\right) \\cdot \\left({k}_1+1\\right)}{f\\left({q}_i,\\kern0.5em CD\\right) + {k}_1 \\cdot \\Big(1-b + b \\cdot \\frac{\\left|CD\\right|}{avgdl}} $\n",
    "- the function *bm*25(*CD*, *QD*) computes the BM25 rank or score of the document *CD* based on the query document *QD*.\n",
    "- The function ${idf}(q_i)$  gives us the *inverse document frequency* (IDF) of the term $q_i$ in the corpus that contains *CD* and from which we want to retrieve the relevant documents.\n",
    "\n",
    "$\\large idf(t)=1 + \\log \\frac{C}{1+df(t)} $\n",
    "- **${idf}(t)$** :\n",
    "  - where `idf(t)` represents the `idf` for the term `t` \n",
    "  - and `C` represents the count of the total number of documents in our corpus \n",
    "  - and `df(t)` represents the frequency of the number of documents in which the term `t`is present.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 implementation steps :\n",
    "There are several steps we must go through to successfully implement and compute BM25 scores for documents:\n",
    "1. Build a function to get inverse document frequency (IDF) values for terms in corpus.\n",
    "2. Build a function for computing BM25 scores for query document and corpus documents.\n",
    "3. Get Bag of Words–based features for corpus documents and query documents.\n",
    "4. Compute average length of corpus documents and IDFs of the terms in the corpus documents using function from point 1.\n",
    "5. Compute BM25 scores, rank relevant documents, and fetch the *n* most relevant documents for each query document using the function in point 2.\n",
    "\n",
    "참고 :\n",
    "- [BM25 vs Lucene Default Similarity](https://www.elastic.co/kr/blog/found-bm-vs-lucene-default-similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step1 : idf for terms in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "def compute_corpus_term_idfs(corpus_features, norm_corpus):\n",
    "    dfs = np.diff(sp.csc_matrix(corpus_features, copy=True).indptr)\n",
    "    dfs = 1 + dfs # to smoothen idf later\n",
    "    total_docs = 1 + len(norm_corpus)\n",
    "    idfs = 1.0 + np.log(float(total_docs) / dfs)\n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2 : bm25 similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bm25_similarity(doc_features, corpus_features, corpus_doc_lengths, avg_doc_length, term_idfs, k1=1.5, b=0.75, top_n=3):\n",
    "    # get corpus bag of words features\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    \n",
    "    # convert query document features to binary features\n",
    "    # this is to keep a note of which terms exist per document\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    doc_features[doc_features >= 1] = 1\n",
    "    \n",
    "    # compute the document idf scores for present terms\n",
    "    doc_idfs = doc_features * term_idfs\n",
    "    \n",
    "    # compute numerator expression in BM25 equation\n",
    "    numerator_coeff = corpus_features * (k1 + 1)\n",
    "    numerator = np.multiply(doc_idfs, numerator_coeff)\n",
    "    \n",
    "    # compute denominator expression in BM25 equation\n",
    "    denominator_coeff =  k1 * (1 - b + (b * (corpus_doc_lengths / avg_doc_length)))\n",
    "    denominator_coeff = np.vstack(denominator_coeff)\n",
    "    denominator = corpus_features + denominator_coeff\n",
    "    \n",
    "    # compute the BM25 score combining the above equations\n",
    "    bm25_scores = np.sum(np.divide(numerator, denominator), axis=1)\n",
    "    \n",
    "    # get top n relevant docs with highest BM25 score                     \n",
    "    top_docs = bm25_scores.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(bm25_scores[index], 3)) for index in top_docs]\n",
    "    \n",
    "    return top_docs_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3\n",
    "vectorizer, corpus_features = build_feature_matrix(norm_corpus, feature_type='frequency')\n",
    "query_docs_features = vectorizer.transform(norm_query_docs)\n",
    "\n",
    "# step4\n",
    "doc_lengths = [len(doc.split()) for doc in norm_corpus]   \n",
    "avg_dl = np.average(doc_lengths) \n",
    "corpus_term_idfs = compute_corpus_term_idfs(corpus_features, norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Analysis using BM25\n",
      "============================================================\n",
      "Document 1 : The fox is definitely smarter than the dog\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 8\n",
      "  BM25 Score: 7.334\n",
      "  Doc : The dog is smarter than the fox\n",
      "----------------------------------------\n",
      "  Doc num: 7\n",
      "  BM25 Score: 3.88\n",
      "  Doc : The fox is quicker than the lazy dog\n",
      "----------------------------------------\n",
      "\n",
      "Document 2 : Java is a static typed programming language unlike Python\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 5\n",
      "  BM25 Score: 7.248\n",
      "  Doc : Python and Java are popular Programming languages\n",
      "----------------------------------------\n",
      "  Doc num: 6\n",
      "  BM25 Score: 6.042\n",
      "  Doc : Among Programming languages, both Python and Java are the most used in Analytics\n",
      "----------------------------------------\n",
      "\n",
      "Document 3 : I love to relax under the beautiful blue sky!\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "  Doc num: 2\n",
      "  BM25 Score: 7.334\n",
      "  Doc : The sky is blue and beautiful\n",
      "----------------------------------------\n",
      "  Doc num: 1\n",
      "  BM25 Score: 4.984\n",
      "  Doc : The sky is blue\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step5                 \n",
    "print ('Document Similarity Analysis using BM25')\n",
    "print ('='*60)\n",
    "for index, doc in enumerate(query_docs):    \n",
    "    doc_features = query_docs_features[index]\n",
    "    top_similar_docs = compute_bm25_similarity(doc_features,\n",
    "                                               corpus_features,\n",
    "                                               doc_lengths,\n",
    "                                               avg_dl,\n",
    "                                               corpus_term_idfs,\n",
    "                                               k1=1.5, b=0.75,\n",
    "                                               top_n=2)\n",
    "    print ('Document',index+1 ,':', doc)\n",
    "    print ('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print ('-'*40)\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        print ('  Doc num: {}'.format(doc_index+1))\n",
    "        print ('  BM25 Score: {}'.format(sim_score))\n",
    "        print ('  Doc : {}'.format(toy_corpus[doc_index]))\n",
    "        print ('-'*40)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
