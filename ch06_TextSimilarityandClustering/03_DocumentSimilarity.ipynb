{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity\n",
    "\n",
    "\n",
    "- Cosine similarity\n",
    "- Hellinger-Bhattacharya distance\n",
    "- Okapi BM25 ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization import normalize_corpus\n",
    "from utils import build_feature_matrix\n",
    "import numpy as np\n",
    "\n",
    "# load the toy corpus index\n",
    "toy_corpus = ['The sky is blue',\n",
    "'The sky is blue and beautiful',\n",
    "'Look at the bright blue sky!',\n",
    "'Python is a great Programming language',\n",
    "'Python and Java are popular Programming languages',\n",
    "'Among Programming languages, both Python and Java are the most used in Analytics',\n",
    "'The fox is quicker than the lazy dog',\n",
    "'The dog is smarter than the fox',\n",
    "'The dog, fox and cat are good friends']\n",
    "\n",
    "# load the docs for which we will be measuring similarities\n",
    "query_docs = ['The fox is definitely smarter than the dog',\n",
    "            'Java is a static typed programming language unlike Python',\n",
    "            'I love to relax under the beautiful blue sky!']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and extract features from the toy corpus\n",
    "norm_corpus = normalize_corpus(toy_corpus, lemmatize=True)\n",
    "tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpus,\n",
    "                                                        feature_type='tfidf',\n",
    "                                                        ngram_range=(1, 1),\n",
    "                                                        min_df=0.0, max_df=1.0)                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and extract features from the query corpus\n",
    "norm_query_docs =  normalize_corpus(query_docs, lemmatize=True)            \n",
    "query_docs_tfidf = tfidf_vectorizer.transform(norm_query_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(doc_features, corpus_features,\n",
    "                              top_n=3):\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features,\n",
    "                        corpus_features.T)\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Analysis using Cosine Similarity\n",
      "============================================================\n",
      "Document 1 : The fox is definitely smarter than the dog\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 8 Similarity Score: 1.0\n",
      "Doc: The dog is smarter than the fox\n",
      "----------------------------------------\n",
      "Doc num: 7 Similarity Score: 0.426\n",
      "Doc: The fox is quicker than the lazy dog\n",
      "----------------------------------------\n",
      "\n",
      "Document 2 : Java is a static typed programming language unlike Python\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 5 Similarity Score: 0.837\n",
      "Doc: Python and Java are popular Programming languages\n",
      "----------------------------------------\n",
      "Doc num: 6 Similarity Score: 0.661\n",
      "Doc: Among Programming languages, both Python and Java are the most used in Analytics\n",
      "----------------------------------------\n",
      "\n",
      "Document 3 : I love to relax under the beautiful blue sky!\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 2 Similarity Score: 1.0\n",
      "Doc: The sky is blue and beautiful\n",
      "----------------------------------------\n",
      "Doc num: 1 Similarity Score: 0.72\n",
      "Doc: The sky is blue\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Document Similarity Analysis using Cosine Similarity')\n",
    "print ('='*60)\n",
    "for index, doc in enumerate(query_docs):\n",
    "    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    top_similar_docs = compute_cosine_similarity(doc_tfidf,\n",
    "                                             tfidf_features,\n",
    "                                             top_n=2)\n",
    "    print ('Document',index+1 ,':', doc)\n",
    "    print ('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print ('-'*40)\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        print ('Doc num: {} Similarity Score: {}\\nDoc: {}'.format(doc_index+1,\n",
    "                                                                 sim_score,\n",
    "                                                                 toy_corpus[doc_index]))\n",
    "        print ('-'*40)\n",
    "    print ('') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELLINGER-BHATTACHARYA DISTANCE\n",
    "\n",
    "- Hellinger distance\n",
    "- Bhattacharya distance\n",
    "\n",
    "$$ hbd\\left(u,\\ v\\right) = \\frac{1}{\\sqrt{2}}\\ \\left|\\right|\\sqrt{u} - \\sqrt{v}\\left|\\right|{}_2 $$\n",
    "\n",
    "$$ hbd\\left(u,\\ v\\right) = \\frac{1}{\\sqrt{2}}\\ \\sqrt{{\\displaystyle \\sum_{i=1}^n}{\\left(\\sqrt{u_i} - \\sqrt{v_i}\\right)}^2} $$\n",
    "\n",
    "\n",
    "$$ u=\\left({u}_1,\\ {u}_2,\\dots,\\ {u}_n\\right) $$\n",
    "\n",
    "$$ v=\\left({v}_1,\\ {v}_2,\\dots,\\ {v}_n\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hellinger_bhattacharya_distance(doc_features, corpus_features,\n",
    "                                            top_n=3):\n",
    "    # get document vectors                                            \n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # compute hb distances\n",
    "    distance = np.hstack(\n",
    "                    np.sqrt(0.5 *\n",
    "                            np.sum(\n",
    "                                np.square(np.sqrt(doc_features) - \n",
    "                                          np.sqrt(corpus_features)), \n",
    "                                axis=1)))\n",
    "    # get docs with lowest distance scores                            \n",
    "    top_docs = distance.argsort()[:top_n]\n",
    "    top_docs_with_score = [(index, round(distance[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Analysis using Hellinger-Bhattacharya distance\n",
      "============================================================\n",
      "Document 1 : The fox is definitely smarter than the dog\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 8 Distance Score: 0.0\n",
      "Doc: The dog is smarter than the fox\n",
      "----------------------------------------\n",
      "Doc num: 7 Distance Score: 0.96\n",
      "Doc: The fox is quicker than the lazy dog\n",
      "----------------------------------------\n",
      "\n",
      "Document 2 : Java is a static typed programming language unlike Python\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 5 Distance Score: 0.53\n",
      "Doc: Python and Java are popular Programming languages\n",
      "----------------------------------------\n",
      "Doc num: 4 Distance Score: 0.766\n",
      "Doc: Python is a great Programming language\n",
      "----------------------------------------\n",
      "\n",
      "Document 3 : I love to relax under the beautiful blue sky!\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 2 Distance Score: 0.0\n",
      "Doc: The sky is blue and beautiful\n",
      "----------------------------------------\n",
      "Doc num: 1 Distance Score: 0.602\n",
      "Doc: The sky is blue\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Document Similarity Analysis using Hellinger-Bhattacharya distance')\n",
    "print ('='*60)\n",
    "for index, doc in enumerate(query_docs):\n",
    "    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    top_similar_docs = compute_hellinger_bhattacharya_distance(doc_tfidf,\n",
    "                                             tfidf_features,\n",
    "                                             top_n=2)\n",
    "    print ('Document',index+1 ,':', doc)\n",
    "    print ('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print ('-'*40)\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        print ('Doc num: {} Distance Score: {}\\nDoc: {}'.format(doc_index+1,\n",
    "                                                                 sim_score,\n",
    "                                                                 toy_corpus[doc_index])  )\n",
    "        print ('-'*40)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKAPI BM25 RANKING\n",
    "\n",
    "\n",
    "$$ bm25\\left(CD,\\ QD\\right) = {\\displaystyle \\sum_{i=1}^n}idf\\left({q}_i\\right)\\cdot \\frac{f\\left({q}_i,\\kern0.5em CD\\right) \\cdot \\left({k}_1+1\\right)}{f\\left({q}_i,\\kern0.5em CD\\right) + {k}_1 \\cdot \\Big(1-b + b \\cdot \\frac{\\left|CD\\right|}{avgdl}} $$\n",
    "\n",
    "\n",
    "$$ idf(t)=1 + \\log \\frac{C}{1+df(t)} $$\n",
    "\n",
    "\n",
    "There are several steps we must go through to successfully implement and compute BM25 scores for documents:\n",
    "\n",
    "1. Build a function to get inverse document frequency (IDF) values for terms in corpus.\n",
    "2. Build a function for computing BM25 scores for query document and corpus documents.\n",
    "3. Get Bag of Words–based features for corpus documents and query documents.\n",
    "4. Compute average length of corpus documents and IDFs of the terms in the corpus documents using function from point 1.\n",
    "5. Compute BM25 scores, rank relevant documents, and fetch the *n* most relevant documents for each query document using the function in point 2.\n",
    "\n",
    "참고 :\n",
    "- [BM25 vs Lucene Default Similarity](https://www.elastic.co/kr/blog/found-bm-vs-lucene-default-similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "def compute_corpus_term_idfs(corpus_features, norm_corpus):\n",
    "\n",
    "    dfs = np.diff(sp.csc_matrix(corpus_features, copy=True).indptr)\n",
    "    dfs = 1 + dfs # to smoothen idf later\n",
    "    total_docs = 1 + len(norm_corpus)\n",
    "    idfs = 1.0 + np.log(float(total_docs) / dfs)\n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bm25_similarity(doc_features, corpus_features,\n",
    "                            corpus_doc_lengths, avg_doc_length,\n",
    "                            term_idfs, k1=1.5, b=0.75, top_n=3):\n",
    "    # get corpus bag of words features\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # convert query document features to binary features\n",
    "    # this is to keep a note of which terms exist per document\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    doc_features[doc_features >= 1] = 1\n",
    "    \n",
    "    # compute the document idf scores for present terms\n",
    "    doc_idfs = doc_features * term_idfs\n",
    "    # compute numerator expression in BM25 equation\n",
    "    numerator_coeff = corpus_features * (k1 + 1)\n",
    "    numerator = np.multiply(doc_idfs, numerator_coeff)\n",
    "    # compute denominator expression in BM25 equation\n",
    "    denominator_coeff =  k1 * (1 - b + (b * (corpus_doc_lengths / avg_doc_length)))\n",
    "    denominator_coeff = np.vstack(denominator_coeff)\n",
    "    denominator = corpus_features + denominator_coeff\n",
    "    # compute the BM25 score combining the above equations\n",
    "    bm25_scores = np.sum(np.divide(numerator, denominator), axis=1)\n",
    "    # get top n relevant docs with highest BM25 score                     \n",
    "    top_docs = bm25_scores.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(bm25_scores[index], 3))\n",
    "                                for index in top_docs]\n",
    "    return top_docs_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Analysis using BM25\n",
      "============================================================\n",
      "Document 1 : The fox is definitely smarter than the dog\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 8 BM25 Score: 7.334\n",
      "Doc: The dog is smarter than the fox\n",
      "----------------------------------------\n",
      "Doc num: 7 BM25 Score: 3.88\n",
      "Doc: The fox is quicker than the lazy dog\n",
      "----------------------------------------\n",
      "\n",
      "Document 2 : Java is a static typed programming language unlike Python\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 5 BM25 Score: 7.248\n",
      "Doc: Python and Java are popular Programming languages\n",
      "----------------------------------------\n",
      "Doc num: 6 BM25 Score: 6.042\n",
      "Doc: Among Programming languages, both Python and Java are the most used in Analytics\n",
      "----------------------------------------\n",
      "\n",
      "Document 3 : I love to relax under the beautiful blue sky!\n",
      "Top 2 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 2 BM25 Score: 7.334\n",
      "Doc: The sky is blue and beautiful\n",
      "----------------------------------------\n",
      "Doc num: 1 BM25 Score: 4.984\n",
      "Doc: The sky is blue\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer, corpus_features = build_feature_matrix(norm_corpus, feature_type='frequency')\n",
    "query_docs_features = vectorizer.transform(norm_query_docs)\n",
    "\n",
    "doc_lengths = [len(doc.split()) for doc in norm_corpus]   \n",
    "avg_dl = np.average(doc_lengths) \n",
    "corpus_term_idfs = compute_corpus_term_idfs(corpus_features, norm_corpus)\n",
    "                 \n",
    "print ('Document Similarity Analysis using BM25')\n",
    "print ('='*60)\n",
    "for index, doc in enumerate(query_docs):    \n",
    "    doc_features = query_docs_features[index]\n",
    "    top_similar_docs = compute_bm25_similarity(doc_features,\n",
    "                                               corpus_features,\n",
    "                                               doc_lengths,\n",
    "                                               avg_dl,\n",
    "                                               corpus_term_idfs,\n",
    "                                               k1=1.5, b=0.75,\n",
    "                                               top_n=2)\n",
    "    print ('Document',index+1 ,':', doc)\n",
    "    print ('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print ('-'*40)\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        print ('Doc num: {} BM25 Score: {}\\nDoc: {}'.format(doc_index+1, sim_score, toy_corpus[doc_index])  )\n",
    "        print ('-'*40)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
